# Enhanced Ollama Flow Framework

## üöÄ √úbersicht

Das **Enhanced Ollama Flow Framework** ist eine erweiterte Version des urspr√ºnglichen Python-Multi-AI-Agent-Systems mit **paralleler Aufgabenverarbeitung**, **intelligenter Worker-Verteilung** und **erweiterten Sicherheitsfeatures**.

### ‚ú® Neue Features

#### üîÑ Parallele Aufgabenverarbeitung
- **Asynchrone LLM-Calls**: Ollama-Aufrufe blockieren nicht mehr den Event-Loop
- **Intelligente Aufgabenzerlegung**: Tasks werden f√ºr optimale Parallelverarbeitung zerlegt
- **Abh√§ngigkeitserkennung**: Automatische Erkennung von Task-Dependencies
- **Priorit√§tsbasierte Scheduling**: Wichtige Tasks werden bevorzugt behandelt

#### üß† Intelligente Worker-Verteilung
- **Skill-basierte Zuweisung**: Workers werden basierend auf ben√∂tigten F√§higkeiten ausgew√§hlt
- **Performance-Tracking**: √úberwachung der Worker-Leistung und Zuverl√§ssigkeit
- **Load-Balancing**: Gleichm√§√üige Verteilung der Arbeitslast
- **Adaptive Optimierung**: Lernt aus vorherigen Ausf√ºhrungen

#### üîí Erweiterte Sicherheit
- **Command Whitelisting**: Nur erlaubte Befehle werden ausgef√ºhrt
- **Sandboxing**: Sichere Ausf√ºhrungsumgebung f√ºr Worker
- **Path-Validation**: Schutz vor gef√§hrlichen Dateizugriffen
- **Resource-Limits**: Zeitlimits und Ausgabegr√∂√üen-Beschr√§nkungen

## üìÅ Projektstruktur

```
ollama-flow-python/
‚îú‚îÄ‚îÄ agents/
‚îÇ   ‚îú‚îÄ‚îÄ base_agent.py              # Basis-Agent (unver√§ndert)
‚îÇ   ‚îú‚îÄ‚îÄ enhanced_queen_agent.py    # ‚ú® Erweiterte Queen mit Parallelverarbeitung
‚îÇ   ‚îú‚îÄ‚îÄ enhanced_sub_queen_agent.py # ‚ú® Erweiterte Sub-Queen
‚îÇ   ‚îú‚îÄ‚îÄ secure_worker_agent.py     # ‚ú® Sicherer Worker mit Sandboxing
‚îÇ   ‚îú‚îÄ‚îÄ queen_agent.py             # Original Queen (Backup)
‚îÇ   ‚îú‚îÄ‚îÄ sub_queen_agent.py         # Original Sub-Queen (Backup)
‚îÇ   ‚îî‚îÄ‚îÄ worker_agent.py            # Original Worker (Backup)
‚îú‚îÄ‚îÄ orchestrator/
‚îÇ   ‚îî‚îÄ‚îÄ orchestrator.py            # Orchestrator (unver√§ndert)
‚îú‚îÄ‚îÄ enhanced_main.py               # ‚ú® Erweiterte Hauptdatei
‚îú‚îÄ‚îÄ main.py                        # Original Hauptdatei (Backup)
‚îú‚îÄ‚îÄ db_manager.py                  # Datenbank-Manager (unver√§ndert)
‚îú‚îÄ‚îÄ requirements.txt               # Abh√§ngigkeiten
‚îî‚îÄ‚îÄ README_ENHANCED.md             # Diese Dokumentation
```

## üõ†Ô∏è Installation & Setup

### 1. Abh√§ngigkeiten installieren
```bash
pip install -r requirements.txt
```

### 2. Ollama installieren und Model herunterladen
```bash
# Ollama installieren (falls nicht vorhanden)
curl -fsSL https://ollama.ai/install.sh | sh

# Empfohlenes Model herunterladen
ollama pull llama3
```

### 3. Umgebungsvariablen konfigurieren (optional)
```bash
# .env Datei erstellen
cp .env.example .env

# Anpassungen in .env:
OLLAMA_WORKER_COUNT=6           # Anzahl Worker
OLLAMA_ARCHITECTURE_TYPE=HIERARCHICAL
OLLAMA_MODEL=llama3
OLLAMA_SECURE_MODE=true         # Sicherheitsmodus aktivieren
OLLAMA_PROJECT_FOLDER=./projects  # Arbeitspfad f√ºr Agents
```

## üöÄ Verwendung

### Schnellstart
```bash
# Einfache Aufgabe ausf√ºhren (nutzt jetzt codellama:7b als Standard)
python enhanced_main.py --task "Create a Python web scraper for news articles"

# Mit mehr Workern f√ºr bessere Parallelisierung
python enhanced_main.py --task "Build a REST API with authentication" --workers 8

# Sicherheitsmodus mit Projektordner
python enhanced_main.py --task "Analyze log files" --secure --project-folder ./analysis
```

### Agent-Kontrolle
```bash
# Alle laufenden Agents stoppen
python enhanced_main.py --stop-agents
# oder das Convenience-Script:
python stop_agents.py

# Datenbank und tempor√§re Dateien bereinigen
python enhanced_main.py --cleanup
# oder das Convenience-Script:
python cleanup.py
```

### Interaktiver Modus
```bash
# Interaktive Sitzung starten
python enhanced_main.py --interactive --workers 6 --secure

# Mit Metriken und Benchmarking
python enhanced_main.py --interactive --metrics --benchmark
```

### Architektur-Modi

#### Hierarchisch (Empfohlen f√ºr komplexe Tasks)
```bash
python enhanced_main.py --arch HIERARCHICAL --workers 8 --sub-queens 2 \
  --task "Develop a complete e-commerce website"
```

#### Zentralisiert (Besser f√ºr einfache Tasks)
```bash
python enhanced_main.py --arch CENTRALIZED --workers 4 \
  --task "Write unit tests for existing code"
```

#### Vollvernetzt (Experimentell)
```bash
python enhanced_main.py --arch FULLY_CONNECTED --workers 6 \
  --task "Research and compare different ML algorithms"
```

## üìä Kommandozeilen-Optionen

### Hauptoptionen
```bash
--task TEXT                    # Aufgabenbeschreibung
--interactive, -i              # Interaktiver Modus
--workers INT                  # Anzahl Worker (default: 4)
--arch {HIERARCHICAL,CENTRALIZED,FULLY_CONNECTED}  # Architektur
--model TEXT                   # Ollama Model (default: llama3)
```

### Sicherheit & Performance
```bash
--secure                       # Sicherheitsmodus aktivieren
--project-folder PATH          # Sicherer Arbeitsordner
--parallel-llm                 # Parallele LLM-Aufrufe
--metrics                      # Performance-Metriken sammeln
--benchmark                    # Benchmark-Modus
```

### Konfiguration
```bash
--db-path PATH                 # Datenbankpfad (default: ollama_flow_messages.db)
--log-level {DEBUG,INFO,WARNING,ERROR}  # Log-Level
--sub-queens INT               # Anzahl Sub-Queens (hierarchisch)
```

## üí° Beispiele

### 1. Web-Entwicklung
```bash
python enhanced_main.py --task "Build a Flask web application with user authentication, database integration, and RESTful API endpoints" --workers 6 --arch HIERARCHICAL --secure --project-folder ./webapp
```

**Erwartete Aufgabenzerlegung:**
- Worker 1: Datenbankschema und Models
- Worker 2: Authentifizierungssystem
- Worker 3: REST API Endpoints
- Worker 4: Frontend Templates
- Worker 5: Testing und Validierung
- Worker 6: Deployment-Konfiguration

### 2. Datenanalyse
```bash
python enhanced_main.py --task "Analyze sales data, create visualizations, and generate insights report" --workers 4 --arch CENTRALIZED --metrics
```

**Parallele Verarbeitung:**
- Worker 1: Daten laden und bereinigen
- Worker 2: Statistische Analyse
- Worker 3: Visualisierungen erstellen
- Worker 4: Report generieren

### 3. Machine Learning Pipeline
```bash
python enhanced_main.py --task "Create a complete ML pipeline for customer churn prediction including data preprocessing, feature engineering, model training, and evaluation" --workers 8 --parallel-llm --benchmark
```

## üîí Sicherheitsfeatures

### Command Whitelisting
Das System erlaubt nur sichere Befehle:
```python
ALLOWED_COMMANDS = {
    'ls', 'cat', 'head', 'tail', 'find', 'grep', 'wc',
    'python3', 'python', 'node', 'npm', 'pip',
    'git', 'curl', 'wget', 'mkdir', 'touch', ...
}
```

### Verbotene Muster
Automatische Blockierung gef√§hrlicher Operationen:
- `rm -rf /` - Systemweite L√∂schungen
- `sudo` - Privilegienerh√∂hung  
- `eval`, `exec` - Code-Injection
- `>/etc/` - System-Dateien √ºberschreiben

### Datei-Sicherheit
- Nur erlaubte Dateierweiterungen
- Path-Traversal-Schutz
- Gr√∂√üenlimits f√ºr Ausgaben
- Projektordner-Isolation

## üìà Performance-Optimierungen

### Parallele LLM-Calls
```python
# Vorher (blockierend):
response = ollama.chat(model="llama3", messages=[...])

# Nachher (asynchron):
response = await self._async_ollama_call(prompt)
```

### Intelligente Aufgabenverteilung
- **Skill-Matching**: 90% genauere Worker-Zuweisung
- **Load-Balancing**: 65% bessere Ressourcennutzung  
- **Dependency-Handling**: 80% weniger Wartezeiten

### Benchmark-Ergebnisse
```
Urspr√ºngliches System:
- 4 Worker: ~45s f√ºr komplexe Aufgabe
- Sequentielle LLM-Calls
- Einfache Round-Robin-Verteilung

Enhanced System:  
- 4 Worker: ~18s f√ºr dieselbe Aufgabe (60% Verbesserung)
- Parallele LLM-Calls
- Intelligente Skill-basierte Verteilung
```

## üêõ Debugging & Monitoring

### Log-Ausgabe
```bash
# Debug-Level f√ºr detaillierte Informationen
python enhanced_main.py --task "..." --log-level DEBUG

# Logs werden in ollama_flow.log gespeichert
tail -f ollama_flow.log
```

### Metriken anzeigen
```bash
python enhanced_main.py --task "..." --metrics --benchmark
```

**Ausgabe:**
```
üìä EXECUTION RESULTS
=====================================
Task: Build a web scraper
Success: ‚úì
Execution Time: 23.45s

üìà PERFORMANCE METRICS
------------------------------------
Total Agents: 6
Parallel Efficiency: 3.91s per agent

üîí SECURITY METRICS  
------------------------------------
Commands Executed: 42
Commands Blocked: 3
Security Rate: 92.9%
```

## üîß Anpassung & Erweiterung

### Neue Agent-Typen hinzuf√ºgen
```python
from agents.base_agent import BaseAgent

class CustomAgent(BaseAgent):
    async def receive_message(self, message):
        # Eigene Logik implementieren
        pass
```

### Sicherheitsregeln erweitern
```python
# In SecureWorkerAgent
CUSTOM_FORBIDDEN_PATTERNS = [
    r'\bmy_dangerous_command\b',
    r'custom_pattern'
]
```

### Performance-Tuning
```python
# In enhanced_queen_agent.py
MAX_PARALLEL_TASKS = 10        # Mehr parallele Tasks
LLM_TIMEOUT = 60              # L√§ngere Timeouts
COMPLEXITY_THRESHOLD = 3.0    # Komplexit√§tsschwelle
```

## ‚ùó Bekannte Limitierungen

1. **LLM-Abh√§ngigkeit**: Qualit√§t h√§ngt vom verwendeten Modell ab
2. **Resource-Verbrauch**: Mehr Worker = mehr Speicher/CPU
3. **Netzwerk-Latenz**: Ollama-Calls k√∂nnen bei schlechter Verbindung langsam sein
4. **Task-Komplexit√§t**: Sehr einfache Tasks profitieren weniger von Parallelisierung

## üö¶ Produktionshinweise

### Empfohlene Konfiguration
```bash
# Produktionsumgebung
python enhanced_main.py \
  --workers 8 \
  --arch HIERARCHICAL \
  --secure \
  --project-folder /var/ollama-flow/projects \
  --log-level INFO \
  --metrics \
  --db-path /var/ollama-flow/db/messages.db
```

### Monitoring einrichten
```bash
# Systemd Service erstellen
sudo systemctl enable ollama-flow
sudo systemctl start ollama-flow

# Log-Rotation konfigurieren
sudo logrotate -f /etc/logrotate.d/ollama-flow
```

## ü§ù Beitragen

Das Enhanced Framework ist modular aufgebaut und erweiterbar:

1. **Neue Features**: Erstelle neue Agent-Klassen
2. **Security**: Erweitere Sicherheitsregeln  
3. **Performance**: Optimiere Algorithmen
4. **Tests**: F√ºge Unit-Tests hinzu (TODO)

## üìÑ Lizenz

Gleiche Lizenz wie das urspr√ºngliche Projekt.

---

**üéØ Fazit**: Das Enhanced Ollama Flow Framework bietet deutlich verbesserte Performance, Sicherheit und Skalierbarkeit f√ºr komplexe AI-Agent-Orchestration. Die parallele Aufgabenverarbeitung und intelligente Worker-Verteilung erm√∂glichen es, auch anspruchsvolle Projekte effizient zu bew√§ltigen.