#!/bin/bash

# Einfacher CLI Wrapper f√ºr ollama-flow
# Funktioniert sofort ohne ZLUDA Setup

set -e

# Farben
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
RED='\033[0;31m'
NC='\033[0m'

# Projekt-Verzeichnis
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_DIR="$SCRIPT_DIR"

print_info() {
    echo -e "${BLUE}‚ÑπÔ∏è  $1${NC}"
}

print_success() {
    echo -e "${GREEN}‚úÖ $1${NC}"
}

print_error() {
    echo -e "${RED}‚ùå $1${NC}"
}

print_warning() {
    echo -e "${YELLOW}‚ö†Ô∏è  $1${NC}"
}

show_help() {
    echo -e "${BLUE}üöÄ Ollama Flow Simple CLI${NC}"
    echo ""
    echo "Befehle:"
    echo "  run <task>               - Task ausf√ºhren"
    echo "  run-zluda <task>         - Task mit ZLUDA (falls verf√ºgbar)"
    echo "  status                   - Backend Status pr√ºfen"
    echo "  models                   - Verf√ºgbare Modelle anzeigen"
    echo "  test                     - System testen"
    echo "  help                     - Diese Hilfe"
    echo ""
    echo "Beispiele:"
    echo "  $0 run \"Erstelle eine Python Flask App\" --workers 2"
    echo "  $0 run-zluda \"Complex task\" --workers 4"
    echo "  $0 status"
    echo ""
}

run_task() {
    local task="$1"
    shift
    
    if [ -z "$task" ]; then
        print_error "Keine Task angegeben"
        return 1
    fi
    
    print_info "F√ºhre Task aus: $task"
    
    # Parameter konvertieren
    local args=()
    while [[ $# -gt 0 ]]; do
        case $1 in
            --workers)
                args+=(--worker-count "$2")
                shift 2
                ;;
            --arch)
                args+=(--architecture-type "$2")
                shift 2
                ;;
            --model)
                args+=(--ollama-model "$2")
                shift 2
                ;;
            *)
                args+=("$1")
                shift
                ;;
        esac
    done
    
    cd "$PROJECT_DIR"
    python main.py \
        --task "$task" \
        --architecture-type HIERARCHICAL \
        --ollama-model codellama:7b \
        --project-folder "$(pwd)" \
        "${args[@]}"
}

run_task_zluda() {
    local task="$1"
    shift
    
    if [ -z "$task" ]; then
        print_error "Keine Task angegeben"
        return 1
    fi
    
    print_info "F√ºhre Task mit ZLUDA aus: $task"
    
    # Parameter konvertieren
    local args=()
    while [[ $# -gt 0 ]]; do
        case $1 in
            --workers)
                args+=(--worker-count "$2")
                shift 2
                ;;
            --arch)
                args+=(--architecture-type "$2")
                shift 2
                ;;
            --model)
                args+=(--ollama-model "$2")
                shift 2
                ;;
            *)
                args+=("$1")
                shift
                ;;
        esac
    done
    
    cd "$PROJECT_DIR"
    python main.py \
        --use-zluda \
        --task "$task" \
        --architecture-type HIERARCHICAL \
        --ollama-model codellama:7b \
        --project-folder "$(pwd)" \
        "${args[@]}"
}

check_status() {
    print_info "Pr√ºfe Backend Status..."
    
    cd "$PROJECT_DIR"
    python -c "
from llm_backend import backend_manager
print('Verf√ºgbare Backends:')
backends = backend_manager.get_available_backends()
for name, available in backends.items():
    status = '‚úÖ' if available else '‚ùå'
    print(f'  {status} {name}')

print('\nModelle:')
try:
    models = backend_manager.get_models()
    if models:
        for model in models[:5]:  # Erste 5 Modelle
            print(f'  - {model}')
        if len(models) > 5:
            print(f'  ... und {len(models) - 5} weitere')
    else:
        print('  Keine Modelle gefunden')
except:
    print('  Fehler beim Laden der Modelle')
"
}

list_models() {
    print_info "Verf√ºgbare Modelle:"
    
    # Ollama Modelle
    if command -v ollama &> /dev/null; then
        echo "Ollama Modelle:"
        ollama list 2>/dev/null | tail -n +2 | awk '{print "  - " $1}' || echo "  Keine Ollama Modelle gefunden"
    else
        echo "  Ollama nicht installiert"
    fi
    
    echo ""
    echo "GGUF Modelle (f√ºr ZLUDA):"
    if [ -d "$PROJECT_DIR/models" ]; then
        find "$PROJECT_DIR/models" -name "*.gguf" -exec basename {} \; 2>/dev/null | sed 's/^/  - /' || echo "  Keine GGUF Modelle gefunden"
    else
        echo "  Models-Verzeichnis nicht gefunden"
    fi
}

run_test() {
    print_info "Teste System..."
    
    cd "$PROJECT_DIR"
    python -c "
import asyncio
from llm_backend import backend_manager

async def test():
    print('üß™ Backend Test...')
    backends = backend_manager.get_available_backends()
    
    for name, available in backends.items():
        if available:
            print(f'‚úÖ {name} Backend verf√ºgbar')
            try:
                response = await backend_manager.chat_with_fallback(
                    messages=[{'role': 'user', 'content': 'Hallo, kannst du \"Test erfolgreich\" sagen?'}],
                    model='codellama:7b' if name == 'ollama' else 'tinyllama-1',
                    backend=name,
                    timeout=15.0
                )
                print(f'   Antwort: {response.content[:50]}...')
                return True
            except Exception as e:
                print(f'   ‚ùå Test fehlgeschlagen: {e}')
        else:
            print(f'‚ùå {name} Backend nicht verf√ºgbar')
    
    print('‚ùå Kein Backend funktioniert')
    return False

if asyncio.run(test()):
    print('\\n‚úÖ System Test erfolgreich!')
else:
    print('\\n‚ùå System Test fehlgeschlagen!')
"
}

main() {
    local command="${1:-help}"
    shift 2>/dev/null || true
    
    case "$command" in
        "run")
            run_task "$@"
            ;;
        "run-zluda")
            run_task_zluda "$@"
            ;;
        "status")
            check_status
            ;;
        "models")
            list_models
            ;;
        "test")
            run_test
            ;;
        "help"|"-h"|"--help")
            show_help
            ;;
        *)
            print_error "Unbekannter Befehl: $command"
            show_help
            exit 1
            ;;
    esac
}

main "$@"